{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQL on Chrome's Dino Run game\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "# image processing\n",
    "import cv2\n",
    "import base64\n",
    "from io import BytesIO\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# selenium and drivers\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# ML (keras + tensorflow)\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import tensorflow as tf\n",
    "from helpers import load_pkl, save_pkl\n",
    "\n",
    "# For more repetitive results\n",
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting notebook variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISCOUNT = 0.99\n",
    "REPLAY_MEMORY_SIZE = 50000  # How many last steps to keep for model training\n",
    "MIN_REPLAY_MEMORY_SIZE = 1000  # Minimum number of steps in a memory to start training\n",
    "MINIBATCH_SIZE = 32  # How many steps (samples) to use for training\n",
    "UPDATE_TARGET_EVERY = 5  # Terminal states (end of episodes)\n",
    "MODEL_NAME = '2x256'\n",
    "\n",
    "# Exploration settings\n",
    "epsilon = 1  # not a constant, going to be decayed\n",
    "EPSILON_DECAY = 0.999\n",
    "MIN_EPSILON = 1e-3\n",
    "\n",
    "# Model settings\n",
    "LEARNING_RATE = 1e-2\n",
    "\n",
    "#  Stats settings\n",
    "AGGREGATE_STATS_EVERY = 20  # episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is written in python and the game is built in JavaScript, so we need some interfacing tools for them to communicate with each other. Using the `selenium`-library, a popular browser automation tool, we can execute actions in the browser (like pressing a key), and extract information from the browser. The `Game` class is a selenium interface between this script and the browser running the Dino game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game: \n",
    "    def __init__(self):\n",
    "        options = webdriver.chrome.options.Options()\n",
    "        options.add_argument(\"disable-infobars\")\n",
    "        options.add_argument(\"--mute-audio\")\n",
    "        self.driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "        self.driver.get(\"chrome://dino\")\n",
    "        self.driver.execute_script(\"Runner.config.ACCELERATION=0\")\n",
    "        self.driver.execute_script(\"document.getElementsByClassName('runner-canvas')[0].id = 'runner-canvas'\")\n",
    "    \n",
    "    def __exec(self, command):\n",
    "        self.driver.execute_script(\"Runner.instance_.{}\".format(command))\n",
    "        \n",
    "    def __get_val(self, value): \n",
    "        return self.driver.execute_script(\"return Runner.instance_.{}\".format(value))\n",
    "    \n",
    "    def press_key(self, key):\n",
    "        self.driver.find_element_by_tag_name(\"body\").send_keys(key)\n",
    "        \n",
    "    def pause(self):\n",
    "        self.__exec(\"stop()\")\n",
    "        \n",
    "    def resume(self):\n",
    "        self.__exec(\"play()\")\n",
    "        \n",
    "    def restart(self):\n",
    "        self.__exec(\"restart()\")\n",
    "    \n",
    "    def get_crashed(self):\n",
    "        return self.__get_val(\"crashed\")\n",
    "    \n",
    "    def get_playing(self):\n",
    "        return self.__get_val(\"playing\")\n",
    "    \n",
    "    def get_score(self):\n",
    "        return int(''.join(self.__get_val(\"distanceMeter.digits\")))\n",
    "    \n",
    "    def get_driver(self):\n",
    "        return self.driver\n",
    "    \n",
    "    def end_game(self):\n",
    "        self.driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Player` is a convenience class that can be seen as the dinosaur itself. It is simply a proxy to the `Game` class, so all actions performed by the dinosaur will go through this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player: \n",
    "    def __init__(self, game): \n",
    "        self._game = game\n",
    "            \n",
    "    def do_action(self, choice):\n",
    "        if choice == 0:  # walk\n",
    "            pass\n",
    "        \n",
    "        if choice == 1:  # jump\n",
    "            self._game.press_key(Keys.ARROW_UP)\n",
    "            \n",
    "        if choice == 2:  # duck\n",
    "            self._game.press_key(Keys.ARROW_DOWN)\n",
    "            \n",
    "    def is_running(self):\n",
    "        return self._game.get_playing()\n",
    "    \n",
    "    def is_crashed(self):\n",
    "        return self._game.get_crashed()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `GameEnv` class defines the environment and handles state progression (through the `step`-function). This class is also responsible for capturing screenshots, which are the inputs to our models. We use the the OpenCV library to capture screenshots, as this is what was found as the most efficient library for this task. The raw image captured has a resolution of around 600x150 x 3 channels (RGB). We intend to use four consecutive images as inputs to our model, which gives a total dimension of 600x150x3x4 per input. To reduce computational demand, we crop, recolor and resize the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameEnv:\n",
    "    MOVE_REWARD = 1\n",
    "    CRASH_PENALTY = -10\n",
    "    IMAGE_SIZE = (84, 84)\n",
    "    OBSERVATION_SPACE_SIZE = (*IMAGE_SIZE, 4)\n",
    "    ACTION_SPACE_SIZE = 2\n",
    "    \n",
    "    def __init__(self, game, player):\n",
    "        self._game = game\n",
    "        self._player = player\n",
    "        \n",
    "        self.current_state = self.get_initial_state()\n",
    "            \n",
    "    def get_screenshot(self):\n",
    "        getbase64Script = \"canvasRunner = document.getElementById('runner-canvas');\\\n",
    "        return canvasRunner.toDataURL().substring(22)\"\n",
    "\n",
    "        # take screenshot\n",
    "        image_b64 = self._game.driver.execute_script(getbase64Script)\n",
    "        screen = np.array(Image.open(BytesIO(base64.b64decode(image_b64))))\n",
    "\n",
    "        # color, crop and resize\n",
    "        image = cv2.cvtColor(screen, cv2.COLOR_RGB2GRAY)\n",
    "        image = image[:300, :600]\n",
    "        image = cv2.resize(image, self.IMAGE_SIZE)\n",
    "    \n",
    "        return image\n",
    "    \n",
    "    def get_initial_state(self):\n",
    "        observation = self.get_screenshot()\n",
    "        return np.stack((observation, observation, observation, observation), axis=2)\n",
    "\n",
    "    def step(self, action):\n",
    "        self._player.do_action(action)\n",
    "        \n",
    "        reward = self.MOVE_REWARD\n",
    "        done = False\n",
    "        \n",
    "        new_observation = self.get_screenshot()  # (80, 80)\n",
    "        new_state = np.append(new_observation.reshape(*new_observation.shape, 1), self.current_state[:,:,:-1], axis=2)\n",
    "        self.current_state = new_state\n",
    "\n",
    "        if self._player.is_crashed():\n",
    "            reward = self.CRASH_PENALTY\n",
    "            done = True\n",
    "\n",
    "        return new_state, reward, done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `DQNAgent` class handles the network and training of the network. Some things to notice about this class: \n",
    "- We define _two_ models: `model` and `target_model`. Every step we make in an episode, we want to update the Q-values (fit), but we are also trying to predict with our model. This causes _a lot_ of fit-operations and predicts, which intuitively can be quite confusing. To ensure some consistency, we therefore define two models - one that is fit every step, and one that predicts every step. `target_model` is updated with `model`'s weights every nth step.  \n",
    "\n",
    "- Model inputs are 4 images (screenshots) with size 84x84. Size = (84, 84, 4).\n",
    "- Model outputs are 2 neurons, representing the maximum predicted reward for each action.\n",
    "\n",
    "- We use `replay_memory` to store transitions of form (current_state, action, reward, new_current_state, done). We store up to 50 000 transitions.\n",
    "\n",
    "- We only train in batches. So for every step, we sample a `MINIBATCH_SIZE` of observations from `replay_memory`, and fit our model on this batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, env):\n",
    "\n",
    "        # Main model\n",
    "        self.model = self.create_model()\n",
    "\n",
    "        # Target network (updated every nth step)\n",
    "        self.target_model = self.create_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "        # An array with last n steps for training\n",
    "        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)\n",
    "\n",
    "        # Used to count when to update target network with main network's weights\n",
    "        self.target_update_counter = 0\n",
    "        \n",
    "        self.env = env\n",
    "    \n",
    "        \n",
    "    def create_model(self):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Conv2D(32, (8, 8), padding='same',strides=(4, 4),input_shape=env.OBSERVATION_SPACE_SIZE))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (4, 4), strides=(2, 2), padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Conv2D(64, (3, 3), strides=(1, 1), padding='same'))\n",
    "        model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(64))\n",
    "        model.add(Activation('relu'))\n",
    "        \n",
    "        model.add(Dense(env.ACTION_SPACE_SIZE))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=LEARNING_RATE))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def update_replay_memory(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "    \n",
    "    # Trains main network every step during episode\n",
    "    def train(self, terminal_state, step):\n",
    "\n",
    "        # Start training only if certain number of samples is already saved\n",
    "        if len(self.replay_memory) < MIN_REPLAY_MEMORY_SIZE:\n",
    "            return\n",
    "\n",
    "        # Get a minibatch of random samples from memory replay table\n",
    "        minibatch = random.sample(self.replay_memory, MINIBATCH_SIZE)\n",
    "\n",
    "        # Get current states from minibatch, then query NN model for Q values\n",
    "        current_states = np.array([transition[0] for transition in minibatch])\n",
    "        current_qs_list = self.model.predict(current_states/255)\n",
    "\n",
    "        # Get future states from minibatch, then query NN model for Q values\n",
    "        # When using target network, query it, otherwise main network should be queried\n",
    "        new_current_states = np.array([transition[3] for transition in minibatch])\n",
    "        future_qs_list = self.target_model.predict(new_current_states/255)\n",
    "\n",
    "        X, y = [], []\n",
    "        \n",
    "        for index, (current_state, action, reward, new_current_state, done) in enumerate(minibatch):\n",
    "\n",
    "            # If not a terminal state, get new q from future states, otherwise set it to 0\n",
    "            if not done:\n",
    "                max_future_q = np.max(future_qs_list[index])\n",
    "                new_q = reward + DISCOUNT * max_future_q\n",
    "            else:\n",
    "                new_q = reward\n",
    "\n",
    "            # Update Q value for given state\n",
    "            current_qs = current_qs_list[index]\n",
    "            current_qs[action] = new_q\n",
    "\n",
    "            # And append to our training data\n",
    "            X.append(current_state)\n",
    "            y.append(current_qs)\n",
    "            \n",
    "        # Fit on all samples as one batch\n",
    "        self.model.fit(np.array(X)/255, np.array(y), batch_size=MINIBATCH_SIZE, verbose=0, shuffle=False, callbacks=None)\n",
    "\n",
    "        # Update target network counter every episode\n",
    "        if terminal_state:\n",
    "            self.target_update_counter += 1\n",
    "\n",
    "        # If counter reaches set value, update target network with weights of main network\n",
    "        if self.target_update_counter >= UPDATE_TARGET_EVERY:\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "            self.target_update_counter = 0\n",
    "                \n",
    "    def get_qs(self, state):\n",
    "        return self.model.predict(np.array(state).reshape(-1, *state.shape)/255)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main run function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "player = Player(game)\n",
    "env = GameEnv(game, player)\n",
    "agent = DQNAgent(env)\n",
    "\n",
    "# CONTINUE_FROM_CHECKOUT decides whether you want to \"start over\" or continue from latest checkpoint.\n",
    "# NB: This is not \"fail safe\", meaning that it does not check if the files exist. \n",
    "# You must therefore run once with this variable set to False to initialize files.\n",
    "CONTINUE_FROM_CHECKPOINT = False  \n",
    "\n",
    "def initialize_file_structure():\n",
    "    if not os.path.isdir('./models'): \n",
    "        os.makedirs('./models')\n",
    "    if not os.path.isdir('./stats'): \n",
    "        os.makedirs('./stats')\n",
    "    if not os.path.isdir('./store'): \n",
    "        os.makedirs('./store')     \n",
    "\n",
    "initialize_file_structure()\n",
    "\n",
    "if (CONTINUE_FROM_CHECKPOINT):\n",
    "    # load stats\n",
    "    ep_stats = pd.read_csv(f'./stats/{MODEL_NAME}-ep_stats.csv')\n",
    "    ep_rewards = np.load(f'./stats/{MODEL_NAME}-ep_rewards.npy')\n",
    "    ep_scores = np.load(f'./stats/{MODEL_NAME}-ep_scores.npy')\n",
    "    highscore = max(ep_scores)\n",
    "    \n",
    "    # load parameters\n",
    "    notebook_checkpoint = load_pkl(f'./store/{MODEL_NAME}-notebook_checkpoint.pkl')\n",
    "    epsilon = notebook_checkpoint['epsilon']\n",
    "    episode = notebook_checkpoint['episode']\n",
    "    \n",
    "    # load the last saved model\n",
    "    agent.model.load_weights(f'./store/{MODEL_NAME}-latest_weights.h5')\n",
    "    agent.target_model.load_weights(f'./store/{MODEL_NAME}-latest_weights.h5')\n",
    "    \n",
    "    # load replay memory\n",
    "    agent.replay_memory = load_pkl(f'./store/{MODEL_NAME}-replay_memory.pkl')\n",
    "    \n",
    "else: \n",
    "    ep_stats = pd.DataFrame([], columns=[\"episode\", \"min_reward\", \"max_reward\", \"avg_reward\", \"min_score\", \"max_score\", \"avg_score\", \"epsilon\"])\n",
    "    ep_rewards, ep_scores = np.array([]), np.array([])\n",
    "    highscore = -200\n",
    "    episode = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPISODES = 60\n",
    "\n",
    "# Iterate over episodes\n",
    "for ep in tqdm(range(episode+1, EPISODES+1), ascii=True, unit='episodes'):\n",
    "\n",
    "    # Update tensorboard step every episode\n",
    "    # agent.tensorboard.step = ep\n",
    "\n",
    "    # Restarting episode - reset episode reward and step number\n",
    "    episode_reward = 0\n",
    "    step = 0\n",
    "\n",
    "    # Reset environment and get initial state\n",
    "    current_state = env.get_initial_state()\n",
    "\n",
    "    # Reset flag and start iterating until episode ends\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # This part stays mostly the same, the change is to query a model for Q values\n",
    "        if np.random.random() > epsilon:\n",
    "            # Get action from Q table\n",
    "            action = np.argmax(agent.get_qs(current_state))\n",
    "        else:\n",
    "            # Get a random action\n",
    "            action = np.random.randint(0, env.ACTION_SPACE_SIZE)\n",
    "\n",
    "        new_state, reward, done = env.step(action)\n",
    "\n",
    "        # Transform new continous state to new discrete state and count reward\n",
    "        episode_reward += reward\n",
    "\n",
    "        # Every step we update replay memory and train main network\n",
    "        agent.update_replay_memory((current_state, action, reward, new_state, done))\n",
    "        agent.train(done, step)\n",
    "\n",
    "        current_state = new_state\n",
    "        step += 1\n",
    "\n",
    "    episode_score = game.get_score()\n",
    "    \n",
    "    # Save model if it is a new highscore\n",
    "    if (episode_score > highscore):\n",
    "        print(f'New highscore: {episode_score}. Saving model...')\n",
    "        agent.model.save(f'./models/{MODEL_NAME}-highscore-{episode_score}.model')\n",
    "        highscore = episode_score\n",
    "    \n",
    "    # Append episode reward to a list and log stats (every given number of episodes)\n",
    "    ep_rewards = np.append(ep_rewards, episode_reward)\n",
    "    ep_scores = np.append(ep_scores, episode_score)\n",
    "    \n",
    "    if not ep % AGGREGATE_STATS_EVERY or ep == 1:\n",
    "        average_reward = ep_rewards[-AGGREGATE_STATS_EVERY:].mean()\n",
    "        min_reward = ep_rewards[-AGGREGATE_STATS_EVERY:].min()\n",
    "        max_reward = ep_rewards[-AGGREGATE_STATS_EVERY:].max()\n",
    "        \n",
    "        avg_score = ep_scores[-AGGREGATE_STATS_EVERY:].mean()\n",
    "        min_score = ep_scores[-AGGREGATE_STATS_EVERY:].min()\n",
    "        max_score = ep_scores[-AGGREGATE_STATS_EVERY:].max()\n",
    "        \n",
    "        ep_stats.loc[len(ep_stats)] = [ep, min_reward, max_reward, average_reward,min_score,max_score,avg_score,epsilon]\n",
    "                \n",
    "        # Save stats\n",
    "        ep_stats.to_csv(f'./stats/{MODEL_NAME}-ep_stats.csv', index=False)\n",
    "        np.save(f'./stats/{MODEL_NAME}-ep_rewards.npy', ep_rewards)\n",
    "        np.save(f'./stats/{MODEL_NAME}-ep_scores.npy', ep_scores)\n",
    "        \n",
    "        # Save notebook checkpoint\n",
    "        notebook_params = {'episode': ep, 'epsilon': epsilon}\n",
    "        save_pkl(f'./store/{MODEL_NAME}-notebook_checkpoint.pkl', notebook_params)\n",
    "        \n",
    "        # Save replay memory\n",
    "        save_pkl(f'./store/{MODEL_NAME}-replay_memory.pkl', agent.replay_memory)\n",
    "        \n",
    "        # Save model weights\n",
    "        agent.model.save_weights(f'./store/{MODEL_NAME}-latest_weights.h5')\n",
    "        \n",
    "        # Save model, but only when min reward is greater or equal a set value\n",
    "        #if min_reward >= MIN_REWARD:\n",
    "        #    agent.model.save(f'./models/{MODEL_NAME}__{max_reward:_>7.2f}max_{average_reward:_>7.2f}avg_{min_reward:_>7.2f}min__{int(time.time())}.model')\n",
    "\n",
    "    # Decay epsilon\n",
    "    if epsilon > MIN_EPSILON:\n",
    "        epsilon *= EPSILON_DECAY\n",
    "        epsilon = max(MIN_EPSILON, epsilon)\n",
    "    \n",
    "    game.restart()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
